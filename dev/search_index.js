var documenterSearchIndex = {"docs":
[{"location":"functions/#Functions","page":"Function reference","title":"Functions","text":"","category":"section"},{"location":"functions/#Data-structures","page":"Function reference","title":"Data structures","text":"","category":"section"},{"location":"functions/","page":"Function reference","title":"Function reference","text":"Modules = [DistributedData]\nPages = [\"structs.jl\"]","category":"page"},{"location":"functions/#DistributedData.Dinfo","page":"Function reference","title":"DistributedData.Dinfo","text":"Dinfo\n\nThe basic structure for working with loaded data, distributed amongst workers. In completeness, it represents a dataset as such:\n\nval is the \"value name\" under which the data are saved in processes. E.g. val=:foo means that there is a variable foo on each process holding a part of the matrix.\nworkers is a list of workers (in correct order!) that hold the data (similar to DArray.pids)\n\n\n\n\n\n","category":"type"},{"location":"functions/#Base-functions","page":"Function reference","title":"Base functions","text":"","category":"section"},{"location":"functions/","page":"Function reference","title":"Function reference","text":"Modules = [DistributedData]\nPages = [\"base.jl\"]","category":"page"},{"location":"functions/#DistributedData.dexec-Tuple{Any,Any,Any}","page":"Function reference","title":"DistributedData.dexec","text":"dexec(val, fn, workers)\n\nExecute a function on workers, taking val as a parameter. Results are not collected. This is optimal for various side-effect-causing computations that are not easily expressible with dtransform.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dexec-Tuple{Dinfo,Any}","page":"Function reference","title":"DistributedData.dexec","text":"dexec(dInfo::Dinfo, fn)\n\nVariant of dexec that works with Dinfo.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmap-Tuple{Array{T,1} where T,Any,Any}","page":"Function reference","title":"DistributedData.dmap","text":"dmap(arr::Vector, fn, workers)\n\nCall a function fn on workers, with a single parameter arriving from the corresponding position in arr.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmapreduce-NTuple{4,Any}","page":"Function reference","title":"DistributedData.dmapreduce","text":"dmapreduce(val, map, fold, workers; prefetch = :all)\n\nA distributed work-alike of the standard mapreduce: Take a function map (a non-modifying transform on the data) and fold (2-to-1 reduction of the results of map), systematically run them on the data described by val distributed on workers, and return the final reduced result.\n\nIt is assumed that the fold operation is associative, but not commutative (as in semigroups). If there are no workers, operation returns nothing (we don't have a monoid to magically conjure zero elements :[ ).\n\nIn the current version, the reduce step is a sequential left fold, executed in the main process. Parameter prefetch says how many futures should be fetched in advance; increasing prefetch improves the throughput but increases memory usage in case the results of map are big.\n\nExample\n\n# compute the mean of all distributed data\nsum,len = dmapreduce(:myData,\n    (d) -> (sum(d),length(d)),\n    ((s1, l1), (s2, l2)) -> (s1+s2, l1+l2),\n    workers())\nprintln(sum/len)\n\nProcessing multiple arguments (a.k.a. \"zipWith\")\n\nThe val here does not necessarily need to refer to a symbol, you can easily pass in a quoted tuple, which will be unquoted in the function parameter. For example, distributed values :a and :b can be joined as such:\n\ndmapreduce(:((a,b)),\n    ((a,b)::Tuple) -> [a b],\n    vcat,\n    workers())\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmapreduce-Tuple{Array{Dinfo,1},Any,Any}","page":"Function reference","title":"DistributedData.dmapreduce","text":"dmapreduce(dInfo1::Dinfo, dInfo2::Dinfo, map, fold)\n\nVariant of dmapreduce that works with more Dinfos at once.  The data must be distributed on the same set of workers, in the same order.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmapreduce-Tuple{Array{T,1} where T,Any,Any,Any}","page":"Function reference","title":"DistributedData.dmapreduce","text":"dmapreduce(vals::Vector, map, fold, workers)\n\nVariant of dmapreduce that works with more distributed variables at once.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmapreduce-Tuple{Dinfo,Any,Any}","page":"Function reference","title":"DistributedData.dmapreduce","text":"dmapreduce(dInfo::Dinfo, map, fold)\n\nDistributed map/reduce (just as the other overload of dmapreduce) that works with Dinfo.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dpmap-Tuple{Any,Vararg{Any,N} where N}","page":"Function reference","title":"DistributedData.dpmap","text":"dpmap(fn, args...; mod = Main, kwargs...)\n\n\"Distributed pool map.\"\n\nA wrapper for pmap from Distributed package that executes the code in the correct module, so that it can access the distributed variables at remote workers. All arguments other than the first function fn are passed to pmap.\n\nThe function fn should return an expression that is going to get evaluated.\n\nExample\n\nusing Distributed\ndpmap(x -> :(computeSomething(someData, $x)), WorkerPool(workers), Vector(1:10))\n\ndi = distributeSomeData()\ndpmap(x -> :(computeSomething($(di.val), $x)), CachingPool(di.workers), Vector(1:10))\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dtransform","page":"Function reference","title":"DistributedData.dtransform","text":"dtransform(dInfo::Dinfo, fn, tgt::Symbol=dInfo.val)::Dinfo\n\nSame as dtransform, but specialized for Dinfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dtransform-2","page":"Function reference","title":"DistributedData.dtransform","text":"dtransform(val, fn, workers, tgt::Symbol=val)\n\nTransform the worker-local distributed data available as val on workers in-place, by a function fn. Store the result as tgt (default val)\n\nExample\n\n# multiply all saved data by 2\ndtransform(:myData, (d)->(2*d), workers())\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.gather_array","page":"Function reference","title":"DistributedData.gather_array","text":"gather_array(dInfo::Dinfo, dim=1; free=false)\n\nDistributed gather_array (just as the other overload) that works with Dinfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.gather_array-2","page":"Function reference","title":"DistributedData.gather_array","text":"gather_array(val::Symbol, workers, dim=1; free=false)\n\nCollect the arrays distributed on workers under value val into an array. The individual arrays are pasted in the dimension specified by dim, i.e. dim=1 is roughly equivalent to using vcat, and dim=2 to hcat.\n\nval must be an Array-based type; the function will otherwise fail.\n\nIf free is true, the val is unscattered after being gathered.\n\nThis preallocates the array for results, and is thus more efficient than e.g. using dmapreduce with vcat for folding.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.get_from-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.get_from","text":"get_from(worker,val)\n\nGet a value val from a remote worker; quoting of val works just as with save_at. Returns a future with the requested value.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.get_val_from-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.get_val_from","text":"get_val_from(worker,val)\n\nShortcut for instantly fetching the future from get_from.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.remove_from-Tuple{Any,Symbol}","page":"Function reference","title":"DistributedData.remove_from","text":"remove_from(worker,sym)\n\nSets symbol sym on worker to nothing, effectively freeing the data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.save_at-Tuple{Any,Symbol,Any}","page":"Function reference","title":"DistributedData.save_at","text":"save_at(worker, sym, val)\n\nSaves value val to symbol sym at worker. sym should be quoted (or contain a symbol). val gets unquoted in the processing and evaluated at the worker, quote it if you want to pass exact command to the worker.\n\nThis is loosely based on the package ParallelDataTransfers, but made slightly more flexible by omitting/delaying the explicit fetches etc. In particular, save_at is roughly the same as ParallelDataTransfers.sendto, and get_val_from works very much like ParallelDataTransfers.getfrom.\n\nReturn value\n\nA future with Nothing that can be fetched to see that the operation has finished.\n\nExamples\n\naddprocs(1)\nsave_at(2,:x,123)       # saves 123\nsave_at(2,:x,myid())    # saves 1\nsave_at(2,:x,:(myid())) # saves 2\nsave_at(2,:x,:(:x))     # saves the symbol :x\n                        # (just :x won't work because of unquoting)\n\nNote: Symbol scope\n\nThe symbols are saved in Main module on the corresponding worker. For example, save_at(1, :x, nothing) will erase your local x variable. Beware of name collisions.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.scatter_array-Tuple{Symbol,Array,Any}","page":"Function reference","title":"DistributedData.scatter_array","text":"scatter_array(sym, x::Array, workers; dim=1)::Dinfo\n\nDistribute roughly equal parts of array x separated on dimension dim among workers into a worker-local variable sym.\n\nReturns the Dinfo structure for the distributed data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.tmp_symbol-Tuple{Dinfo}","page":"Function reference","title":"DistributedData.tmp_symbol","text":"tmp_symbol(dInfo::Dinfo; prefix=\"\", suffix=\"_tmp\")\n\nDecorate the symbol from dInfo with prefix and suffix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.tmp_symbol-Tuple{Symbol}","page":"Function reference","title":"DistributedData.tmp_symbol","text":"tmp_symbol(s::Symbol; prefix=\"\", suffix=\"_tmp\")\n\nDecorate a symbol s with prefix and suffix, to create a good name for a related temporary value.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.unscatter-Tuple{Dinfo}","page":"Function reference","title":"DistributedData.unscatter","text":"unscatter(dInfo::Dinfo)\n\nRemove the loaded data described by dInfo from the corresponding workers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.unscatter-Tuple{Symbol,Any}","page":"Function reference","title":"DistributedData.unscatter","text":"unscatter(sym, workers)\n\nRemove the loaded data from workers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.@remote-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.@remote","text":"@remote module expr\n\nA version of @remote that adds additional choice of the module for scope.\n\n\n\n\n\n","category":"macro"},{"location":"functions/#DistributedData.@remote-Tuple{Any}","page":"Function reference","title":"DistributedData.@remote","text":"@remote expr\n\nIn a function that will get evaluated on a remote worker, this ensures the evaluation scope of the expression expr (usually a variable) is taken on the remote side, preventing namespace clash with the local session.\n\nThis is mainly useful for making the functions from Distributed package (such as pmap and remotecall) work with the data stored by DistributedData package.\n\nInternally, this is handled by wrapping in eval.\n\nExample\n\njulia> save_at(2, :x, 321)\nFuture(2, 1, 162, nothing)\n\njulia> let x=123\n         remotecall_fetch(() -> x + (@remote x), 2)\n       end\n444\n\n\n\n\n\n","category":"macro"},{"location":"functions/#Higher-level-array-operations","page":"Function reference","title":"Higher-level array operations","text":"","category":"section"},{"location":"functions/","page":"Function reference","title":"Function reference","text":"Modules = [DistributedData]\nPages = [\"tools.jl\"]","category":"page"},{"location":"functions/#DistributedData.catmapbuckets-Tuple{Any,Array,Int64,Array{Int64,1}}","page":"Function reference","title":"DistributedData.catmapbuckets","text":"catmapbuckets(fn, a::Array, nbuckets::Int, buckets::Vector{Int}; bucketdim::Int=1)\n\nSame as mapbuckets, except concatenates the bucketing results in the bucketing dimension, thus creating a slightly neater matrix. slicedims is therefore fixed to bucketdim.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.combine_stats-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.combine_stats","text":"combine_stats((s1, sqs1, n1), (s2, sqs2, n2))\n\nHelper for dstat-style functions that just adds up elements in triplets of vectors.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dapply_cols-Tuple{Dinfo,Any,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dapply_cols","text":"dapply_cols(dInfo::Dinfo, fn, columns::Vector{Int})\n\nApply a function fn over columns of a distributed dataset.\n\nfn gets 2 parameters:\n\na data vector for (the whole column saved at one worker)\nindex of the column in the columns array (i.e. a number from 1:length(columns))\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dapply_rows-Tuple{Dinfo,Any}","page":"Function reference","title":"DistributedData.dapply_rows","text":"dapply_rows(dInfo::Dinfo, fn)\n\nApply a function fn over rows of a distributed dataset.\n\nfn gets a single vector parameter for each row to transform.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dcopy-Tuple{Dinfo,Symbol}","page":"Function reference","title":"DistributedData.dcopy","text":"dcopy(dInfo::Dinfo, newName::Symbol)\n\nClone the dataset and store it under a new distributed name newName.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dcount-Tuple{Int64,Dinfo}","page":"Function reference","title":"DistributedData.dcount","text":"dcount(ncats::Int, dInfo::Dinfo)::Vector{Int}\n\nCount the numbers of integer vector values stored in dInfo; assuming the values are in range 1–ncats.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dcount_buckets-Tuple{Int64,Dinfo,Int64,Dinfo}","page":"Function reference","title":"DistributedData.dcount_buckets","text":"dcount_buckets(ncats::Int, dInfo::Dinfo, nbuckets::Int, buckets::Dinfo)::Matrix{Int}\n\nSame as dcount, but counts the items in dInfo bucketed by buckets to produce a matrix of counts, with ncats rows and nbuckets columns.\n\nUseful with distributeFCSFileVector to determine cluster distribution within files.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmedian-Tuple{Dinfo,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dmedian","text":"dmedian(dInfo::Dinfo, columns::Vector{Int})\n\nCompute a median in a distributed fashion, avoiding data transfer and memory capacity that is required to compute the median in the classical way by sorting. All data must be finite and defined. If the median is just between 2 values, the lower one is chosen.\n\nThe algorithm is approximative, searching for a good median by halving interval and counting how many values are below the threshold. iters can be increased to improve precision, each value adds roughly 1 bit to the precision. The default value is 20, which corresponds to precision 10e-6 times the data range.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dmedian_buckets-Tuple{Dinfo,Int64,Dinfo,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dmedian_buckets","text":"dmedian_buckets(dInfo::Dinfo, nbuckets::Int, buckets::Dinfo, columns::Vector{Int}; iters=20)\n\nA version of dmedian that works with the bucketing information (i.e. clusters) from nbuckets and buckets.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dscale-Tuple{Dinfo,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dscale","text":"dscale(dInfo::Dinfo, columns::Vector{Int})\n\nScale the columns in the dataset to have mean 0 and sdev 1.\n\nPrevents creation of NaNs by avoiding division by zero sdevs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dselect","page":"Function reference","title":"DistributedData.dselect","text":"function dselect(dInfo::Dinfo,\n    currentColnames::Vector{String}, selectColnames::Vector{String};\n    tgt=dInfo.val)::Dinfo\n\nConvenience overload of dselect that works with column names.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dselect-2","page":"Function reference","title":"DistributedData.dselect","text":"dselect(dInfo::Dinfo, columns::Vector{Int}; tgt=dInfo.val)\n\nReduce dataset to selected columns, optionally save it under a different name.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dstat-Tuple{Dinfo,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dstat","text":"dstat(dInfo::Dinfo, columns::Vector{Int})::Tuple{Vector{Float64}, Vector{Float64}}\n\nCompute mean and standard deviation of the columns in dataset. Returns a tuple with a vector of means in columns, and a vector of corresponding sdevs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dstat_buckets-Tuple{Dinfo,Int64,Dinfo,Array{Int64,1}}","page":"Function reference","title":"DistributedData.dstat_buckets","text":"dstat_buckets(dInfo::Dinfo, nbuckets::Int, buckets::Dinfo, columns::Vector{Int})::Tuple{Matrix{Float64}, Matrix{Float64}}\n\nA version of dstat that works with bucketing information (e.g. clusters); returns a tuple of matrices.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.mapbuckets-Tuple{Any,Array,Int64,Array{Int64,1}}","page":"Function reference","title":"DistributedData.mapbuckets","text":"mapbuckets(fn, a::Array, nbuckets::Int, buckets::Vector{Int}; bucketdim::Int=1, slicedims=bucketdim)\n\nApply the function fn over array a so that it processes the data by buckets defined by buckets (that contains integers in range 1:nbuckets).\n\nThe buckets are sliced out in dimension specified by bucketdim.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.reduce_extrema-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.reduce_extrema","text":"reduce_extrema(ex1, ex2)\n\nHelper for gathering the minima and maxima of the data. ex1, ex2 are arrays of pairs (min,max), this function combines the arrays element-wise and finds combined minima and maxima.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.update_extrema-NTuple{4,Any}","page":"Function reference","title":"DistributedData.update_extrema","text":"update_extrema(counts, target, lim, mid)\n\nHelper for distributed median computation – returns updated extrema in lims depending on whether the item count in counts of values less than mids is less or higher than targets.\n\n\n\n\n\n","category":"method"},{"location":"functions/#Input/Output","page":"Function reference","title":"Input/Output","text":"","category":"section"},{"location":"functions/","page":"Function reference","title":"Function reference","text":"Modules = [DistributedData]\nPages = [\"io.jl\"]","category":"page"},{"location":"functions/#DistributedData.defaultFiles-Tuple{Any,Any}","page":"Function reference","title":"DistributedData.defaultFiles","text":"defaultFiles(s, pids)\n\nMake a good set of filenames for saving a dataset.\n\n\n\n\n\n","category":"method"},{"location":"functions/#DistributedData.dload","page":"Function reference","title":"DistributedData.dload","text":"dload(dInfo::Dinfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for Dinfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dload-2","page":"Function reference","title":"DistributedData.dload","text":"dload(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nImport the content of symbol sym by each worker specified by pids from the corresponding filename in files.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dstore","page":"Function reference","title":"DistributedData.dstore","text":"dstore(dInfo::Dinfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for Dinfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dstore-2","page":"Function reference","title":"DistributedData.dstore","text":"dstore(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nExport the content of symbol sym by each worker specified by pids to a corresponding filename in files.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dunlink","page":"Function reference","title":"DistributedData.dunlink","text":"dunlink(dInfo::Dinfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for Dinfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#DistributedData.dunlink-2","page":"Function reference","title":"DistributedData.dunlink","text":"dunlink(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nRemove the files created by dstore with the same parameters.\n\n\n\n\n\n","category":"function"},{"location":"slurm/#DistributedData.jl-on-Slurm-clusters","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"","category":"section"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"DistributedData scales well to moderately large computer clusters. As a practical example, you can see the script that was used to process relatively large datasets for GigaSOM, documented in the Supplementary file of the GigaSOM article (click the giaa127_Supplemental_File link below on the page, and find Listing S1 at the end of the PDF).","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"Use of DistributedData with Slurm is similar as with many other distributed computing systems:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"You install Julia and the required packages.\nYou submit a batch (or interactive) task, which runs your Julia script on a node and gives it some information about where to find other worker nodes.\nIn the Julia script, you use ClusterManagers function addprocs_slurm to add the processes, just as with normal addprocs. Similar functions exist for many other task schedulers, including the popular PBS and LSF.\nThe rest of the workflow is unchanged; all functions from DistributedData such as save_at and dmapreduce will work in the cluster just as they worked locally. Performance will vary though – you may want to optimize your algorithm to use as much parallelism as possible (to get lots of performance), load more data in the memory (usually, much more total memory is available in the clusters than on a single computer), but keep an eye on the communication overhead, transferring only the minimal required amount of data as seldom as possible.","category":"page"},{"location":"slurm/#Preparing-the-packages","page":"DistributedData.jl on Slurm clusters","title":"Preparing the packages","text":"","category":"section"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"The easiest way to install the packages is using a single-machine interactive job. On the access node of your HPC, run this command to give you a 60-minute interactive session:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"srun --pty -t60 /bin/bash -","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"(Depending on your cluster setup, it may be benerifical to also specify a partition to which the job should belong to; many clusters provide an interactive partition where the interactive jobs get scheduled faster. To do that, add option -p interactive into the parameters of srun.)","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"When the shell opens (the prompt should change), you can load the Julia module, usually with a command such as this:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"module load lang/Julia/1.3.0","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"(You may consult module spider julia for other possible Julia versions.)","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"After that, start julia and add press ] to open the packaging prompt (you should see (v1.3) pkg> instead of julia>). There you can download and install the required packages:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"add DistributedData, ClusterManagers","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"You may also want to load the packages to precompile them, which saves precious time later in the workflows. Press backspace to return to the \"normal\" Julia shell, and type:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"using DistributedData, ClusterManagers","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"Depending on the package, this may take a while, but should be done in under a minute for most existing packages. Finally, press Ctrl+D twice to exit both Julia and the interactive Slurm job shell.","category":"page"},{"location":"slurm/#Slurm-batch-script","page":"DistributedData.jl on Slurm clusters","title":"Slurm batch script","text":"","category":"section"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"An example Slurm batch script (download) is listed below – save it as run-analysis.batch to your Slurm access node, in a directory that is shared with the workers (usually a \"scratch\" directory; try cd $SCRATCH).","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"#!/bin/bash -l\n#SBATCH -n 128\n#SBATCH -c 1\n#SBATCH -t 10\n#SBATCH --mem-per-cpu 4G\n#SBATCH -J MyDistributedJob\n\nmodule load lang/Julia/1.3.0\n\njulia run-analysis.jl","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"The parameters in the script have this meaning, in order:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"the batch spawns 128 \"tasks\" (ie. spawning 128 separate processes)\neach task uses 1 CPU (you may want more CPUs if you work with actual threads and shared memory)\nthe whole batch takes maximum 10 minutes\neach CPU (in our case each process) will be allocated 4 gigabytes of RAM\nthe job will be visible in the queue as MyDistributedJob\nit will load Julia 1.3.0 module on the workers, so that julia executable is available (you may want to consult the versions availability with your HPC administrators)\nfinally, it will run the Julia script run-analysis.jl","category":"page"},{"location":"slurm/#Julia-script","page":"DistributedData.jl on Slurm clusters","title":"Julia script","text":"","category":"section"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"The run-analysis.jl (download) may look as follows:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"using  Distributed, ClusterManagers, DistributedData\n\n# read the number of available workers from  environment and start the worker processes\nn_workers = parse(Int , ENV[\"SLURM_NTASKS\"])\naddprocs_slurm(n_workers , topology =:master_worker)\n\n# load the required packages on all workers\n@everywhere using DistributedData\n\n# generate a random dataset on all workers\ndataset = dtransform((), _ -> randn(10000,10000), workers(), :myData)\n\n# for demonstration, sum the whole dataset\ntotalResult = dmapreduce(dataset, sum, +)\n\n# do not forget to save the results!\nf = open(\"result.txt\", \"w\")\nprintln(f, totalResult)\nclose(f)","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"Finally, you can start the whole thing with sbatch command executed on the access node:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"sbatch run-analysis.batch","category":"page"},{"location":"slurm/#Collecting-the-results","page":"DistributedData.jl on Slurm clusters","title":"Collecting the results","text":"","category":"section"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"After your tasks gets queued, executed and finished successfully, you may see the result in result.txt. In the meantime, you can entertain yourself by watching squeue, to see e.g. the expected execution time of your batch.","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"Note that you may want to run the analysis in a separate directory, because the logs from all workers are collected in the current path by default. The resulting files may look like this:","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"0 [user@access1 test]$ ls\njob0000.out  job0019.out  job0038.out  job0057.out  job0076.out  job0095.out  job0114.out\njob0001.out  job0020.out  job0039.out  job0058.out  job0077.out  job0096.out  job0115.out\njob0002.out  job0021.out  job0040.out  job0059.out  job0078.out  job0097.out  job0116.out\njob0003.out  job0022.out  job0041.out  job0060.out  job0079.out  job0098.out  job0117.out\njob0004.out  job0023.out  job0042.out  job0061.out  job0080.out  job0099.out  job0118.out\njob0005.out  job0024.out  job0043.out  job0062.out  job0081.out  job0100.out  job0119.out\njob0006.out  job0025.out  job0044.out  job0063.out  job0082.out  job0101.out  job0120.out\njob0007.out  job0026.out  job0045.out  job0064.out  job0083.out  job0102.out  job0121.out\njob0008.out  job0027.out  job0046.out  job0065.out  job0084.out  job0103.out  job0122.out\njob0009.out  job0028.out  job0047.out  job0066.out  job0085.out  job0104.out  job0123.out\njob0010.out  job0029.out  job0048.out  job0067.out  job0086.out  job0105.out  job0124.out\njob0011.out  job0030.out  job0049.out  job0068.out  job0087.out  job0106.out  job0125.out\njob0012.out  job0031.out  job0050.out  job0069.out  job0088.out  job0107.out  job0126.out\njob0013.out  job0032.out  job0051.out  job0070.out  job0089.out  job0108.out  job0127.out\njob0014.out  job0033.out  job0052.out  job0071.out  job0090.out  job0109.out  result.txt        <-- here is the result!\njob0015.out  job0034.out  job0053.out  job0072.out  job0091.out  job0110.out  run-analysis.jl\njob0016.out  job0035.out  job0054.out  job0073.out  job0092.out  job0111.out  run-analysis.sbatch\njob0017.out  job0036.out  job0055.out  job0074.out  job0093.out  job0112.out  slurm-2237171.out\njob0018.out  job0037.out  job0056.out  job0075.out  job0094.out  job0113.out","category":"page"},{"location":"slurm/","page":"DistributedData.jl on Slurm clusters","title":"DistributedData.jl on Slurm clusters","text":"The files job*.out contain the information collected from individual workers' standard outputs, such as the output of println or @info. For complicated programs, this is the easiest way to get out the debugging information, and a simple but often sufficient way to collect benchmarking output (using e.g. @time).","category":"page"},{"location":"#DistributedData.jl-—-simple-work-with-distributed-data","page":"Documentation","title":"DistributedData.jl — simple work with distributed data","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"This packages provides simple Distributed Data manipulation and processing routines for Julia.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"The design of the package and data manipulation approach is deliberately \"imperative\" and \"hands-on\", to allow as much user influence on the actual way the data are moved and stored in the cluster as possible. It is based on the Distributed package and its infrastructure of remote workers. The basic Distributed package functions remotecall and fetch are then wrapper (very lightly) to create a simple yet powerful data manipulation interface.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"There are also various extra functions to easily run distributed data transformations, MapReduce-style algorithms, store and load the data remotely on worker-local storage (e.g. to prevent memory exhaustion) and others.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"To start quickly, you can read the tutorial:","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages=[\"tutorial.md\", \"slurm.md\"]","category":"page"},{"location":"#Functions","page":"Documentation","title":"Functions","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"A full reference to all functions is given here:","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"tutorial/#DistributedData-tutorial","page":"Tutorial","title":"DistributedData tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The primary purpose of this tutorial is to get a basic grasp of the main DistributedData functions and methodology.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For starting up, let's create a few distributed workers and import the package:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using Distributed, DistributedData\n\njulia> addprocs(3)\n2-element Array{Int64,1}:\n 2\n 3\n 4\n\njulia> @everywhere using DistributedData","category":"page"},{"location":"tutorial/#Moving-the-data-around","page":"Tutorial","title":"Moving the data around","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In DistributedData, the storage of distributed data is done in the \"native\" Julia way – the data is stored in normal named variables. Each node holds its own data in an arbitrary set of variables as \"plain data\"; content of these variables is completely independent among nodes.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are two basic data-moving functions:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"save_at, which evaluates a given expression on the remote worker, and stores it in a variable. In particular, save_at(3, :x, 123) is roughly the same as if you would manually connect to the Julia session on the worker 3 and type x = 123.\nget_from, which evaluates a given object on the remote worker and returns a Future that holds the evaluated result. To get the value of x from worker 3, you may call fetch(get_from(3, :x)) to fetch the \"contents\" of that future. (Additionally, there is get_val_from, which calls the fetch for you.)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The use of these functions is quite straightforward:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> save_at(3,:x,123)\nFuture(3, 1, 11, nothing)\n\njulia> get_val_from(3, :x)\n123\n\njulia> get_val_from(4, :x)\nERROR: On worker 4:\nUndefValError: x not defined\n…","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"DistributedData uses quoting to allow you to precisely specify the parts of the code that should be evaluated on the \"main\" Julia process (the one you interact with), and the code that should be evaluated on the remote workers.  Basically, all quoted code is going to get to the workers without any evaluation; all other code is evaluated on the main node.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For example, this picks up the contents of variable x from the remote worker, despite that actual symbol is named as y in the main process:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> y=:x\n:x\n\njulia> get_val_from(3, y)\n123","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This system is used to easily specify that some particular operations (e.g., heavy computations) are going to be executed on the remotes.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To illustrate the difference between quoted and non-quoted code, the following code generates a huge random matrix locally and sends it to the worker, which may not be desired (the data transfer takes a lot of precious time):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> save_at(2, :mtx, randn(1000, 1000))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"On the remote worker 2, this will be executed as something like mtx = [0.384478, 0.763806, -0.885208, …] .","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If you quote the parameter, it is not going to be evaluated on the main worker, but rather goes unevaluated and \"packed\" as an expression to the remote, which unpacks and evaluates it by itself:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> save_at(2, :mtx, :(randn(1000,1000)))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The data transfer is minimized to a few-byte expression randn(1000,1000). On the remote, this is executed properly as mtx = randn(1000, 1000).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This is useful for handling large data – you can easily load giant datasets to the workers without hauling all the data through your computer; very likely also decreasing the risk of out-of-memory problems.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The same principle applies for receiving the data – you can let some of the workers compute a very hard function and download it as follows:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> get_val_from(2, :( computeAnswerToMeaningOfLife() ))\n42","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If the expression in the previous case was not quoted, it would actually cause the main worker to compute the answer, send it to worker 2, and receive it back unchanged, which is likely not what we wanted.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, this way it is very easy to work with multiple variables saved at a single worker – you just reference them in the expression:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> save_at(2,:x,123)\njulia> save_at(2,:y,321)\njulia> get_val_from(2, :(2*x+y))\n567","category":"page"},{"location":"tutorial/#Parallelization-and-synchronization","page":"Tutorial","title":"Parallelization and synchronization","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Operations executed by save_at and get_from are asynchronous by default, which may be both good and bad, depending on the situation. For example, when using save_at, the results of hard-to-compute functions may not yet be saved at the time you need them. Let's demonstrate that on a \"simulated\" slow function:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> save_at(2, :delayed, :(begin sleep(30); 42; end))\nFuture(2, 1, 18, nothing)\n\njulia> get_val_from(2, :delayed)      # the computation is not finished yet, thus the variable is not assigned\nERROR: On worker 2:\nUndefVarError: delayed not defined\n\n# …wait 30 seconds…\n\njulia> get_val_from(2, :delayed)\n42","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The simplest way to prevent such data races is to fetch the future returned from save_at, which correctly waits until the result is properly available on the target worker.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This synchronization is not performed by default, because the non-synchronized behavior allows you to very easily implement parallelism. In particular, you may start multiple asynchronous computations at once, and then wait for all of them to complete to make sure all results are available. Because the operations run asynchronously, they are processed concurrently, thus faster.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To illustrate the difference, the following code distributes some random data and then synchronizes correctly, but is essentially serial:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> @time for w in workers()\n         fetch(save_at(w, :x, :(randn(10000,10000))))\n       end\n  1.073267 seconds (346 allocations: 12.391 KiB)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By spawning the operations first and waiting for all of them later, you can make the code parallel, and usually a few times faster (depending on the number of workers):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> @time map(fetch, [save_at(w, :x, :(randn(10000,10000)))\n                         for w in workers()])\n  0.403235 seconds (44.50 k allocations: 2.277 MiB)\n3-element Array{Nothing,1}:\nnothing\n…","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The same is applicable for retrieving the sub-results in parallel. This example demonstrates that multiple workers can do some work at the same time:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> @time map(fetch, [get_from(i, :(begin sleep(1); myid(); end))\n                         for i in workers()])\n  1.027651 seconds (42.26 k allocations: 2.160 MiB)\n3-element Array{Int64,1}:\n 2\n 3\n 4","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notably, you can even send individual Futures to other workers, allowing the workers to synchronize and transfer the data among each other. This is beneficial for implementing advanced parallel algorithms.","category":"page"},{"location":"tutorial/#Dinfo-handles","page":"Tutorial","title":"Dinfo handles","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Remembering and managing the remote variable names and worker numbers is extremely impractical, especially if you need to maintain multiple variables on various subsets of all available workers at once. DistributedData defines a small Dinfo data structure that keeps that information for you. Many other functions are able to work with Dinfo transparently, instead of the \"raw\" symbols and worker lists.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For example, you can use scatter_array to automatically separate the array-like dataset to roughly-same pieces scattered across multiple workers, and obtain the Dinfo object:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dataset = scatter_array(:myData, randn(1000,3), workers())\nDinfo(:myData, [2, 3, 4])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Dinfo contains the necessary information about the \"contents\" of the distributed dataset: The name of variable used to save it on workers, and IDs of individual workers. The storage of the variables is otherwise same as with the basic data-moving function – you can e.g. manually check the size of the resulting slices on each worker using get_from:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> map(fetch, [get_from(w, :(size($(dataset.val))))\n                   for w in dataset.workers])\n3-element Array{Tuple{Int64,Int64},1}:\n (333, 3)\n (333, 3)\n (334, 3)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Note the $(...) syntax for un-quoting, i.e., inserting evaluated data into quoted expressions.)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Dinfo object is used e.g. by the statistical functions, such as dstat (see below for more examples). dstat just computes means and standard deviations in selected columns of the data:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dstat(dataset, [1,2])\n([-0.029108965193981328, 0.019687519297162222],     # means\n [0.9923669075507301, 0.9768313338000191])          # sdevs","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are three functions for straightforward data management using the Dinfo:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"dcopy for duplicating the data objects on all related workers\nunscatter for removing the data from workers (and freeing the memory)\ngather_array for collecting the array pieces from individual workers and pasting them together (an opposite of scatter_array)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Continuing the previous example, we can copy the data, remove the originals, and gather the copies:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dataset2 = dcopy(dataset, :backup)\nDinfo(:backup, [2, 3, 4])\n\njulia> unscatter(dataset)\n\njulia> get_val_from(2, :myData)\n # nothing\n\njulia> gather_array(dataset2)\n1000×3 Array{Float64,2}:\n  0.241102   0.62638     0.759203\n  0.981085  -1.01467    -0.495331\n -0.439979  -0.884943   -1.62218\n  ⋮","category":"page"},{"location":"tutorial/#Transformations-and-reductions","page":"Tutorial","title":"Transformations and reductions","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are several simplified functions to run parallel computation on the distributed data:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"dtransform processes all worker's parts of the data using a given function and stores the result\ndmapreduce applies (\"maps\") a function to all data parts and reduces (\"folds\") the intermediate results to a single result using another function\ndexec is similar to dtransform, but expects a function that modifies the data in-place (using \"side-effects\"), for increased efficiency in cases such as very small array modifications\ndmap executes a function over the workers, also distributing a vector of values as parameters for that function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For example, dtransform can be used to exponentiate the whole dataset:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dataset = scatter_array(:myData, randn(1000,3), workers())\njulia> get_val_from(dataset.workers[1], :(myData[1,:]))\n3-element Array{Float64,1}:\n -1.0788051465727018\n -0.29710863020942757\n -2.5613834309426546\n\njulia> dtransform(dataset, x -> 2 .^ x)\nDinfo(:myData, [2, 3, 4])\n\njulia> get_val_from(dataset.workers[1], :(myData[1,:]))\n3-element Array{Float64,1}:\n 0.4734207525033287\n 0.8138819001228813\n 0.16941300928000705","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You may have noticed that dtransform returns a new Dinfo object, which we safely discard in this case. You can use dtransform to save the result into another distributed variable (by supplying the new name in an extra argument), in which case the returned Dinfo wraps this new distributed variable. That is useful for easily generating new datasets on all workers, as in the following example:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> anotherDataset = dtransform((), _ ->randn(100), workers(), :newData)\nDinfo(:newData, [2, 3, 4])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Note that the function body does not need to be quoted.)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The dexec function is handy if your transformation does not modify the whole array, but leaves most of it untouched and rewriting it would be a waste of resources. This example multiplies the 5th element of each distributed array part by 42:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dexec(anotherDataset, arr -> arr[5] *= 42)\njulia> gather_array(anotherDataset)[1:6]\n6-element Array{Float64,1}:\n  0.8270400003123709\n -0.10688512653581493\n -1.0462015551052068\n -1.2891453384843214\n 16.429315504503112\n  0.13958421716454797\n  ⋮","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"MapReduce is a handy primitive that is suitable for operations that can \"compress\" the dataset slices into relatively small pieces of data, which can be combined efficiently. For example, this computes the sum of squares of the whole array:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dmapreduce(anotherDataset, x -> sum(x.^2), +)\n8633.94032741762","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, dmap passes each worker a specific value from a given vector, which may be useful in cases when each worker is supposed to do something slightly different with the data (for example, submit them to a different interface or save them as a different file). The results are returned as a vector. This example is rather simplistic:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dmap(Vector(1:length(workers())),\n                   val -> \"Worker number $(val) has ID $(myid())\",\n\t\t   workers())\n3-element Array{String,1}:\n \"Worker number 1 has ID 2\"\n \"Worker number 2 has ID 3\"\n \"Worker number 3 has ID 4\"","category":"page"},{"location":"tutorial/#Persisting-the-data","page":"Tutorial","title":"Persisting the data","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"DistributedData provides support for storing the loaded dataset in each worker's local storage. This is quite beneficial for saving sub-results and various artifacts of the computation process for later use, without unnecessarily wasting main memory.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The available functions are as follows:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"dstore saves the dataset to a disk, such as in dstore(anotherDataset), which, in this case, creates files newData-1.slice to newData-3.slice that contain the respective parts of the dataset. The precise naming scheme can be specified using the files parameter.\ndload loads the data back into memory (again using a Dinfo parameter with dataset description to get the dataset name and the list of relevant workers)\ndunlink removes the corresponding files from the storage","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Apart from saving the data for later use, this provides a relatively easy way to exchange the data among nodes in a HPC environment. There, the disk storage is usually a very fast \"scratch space\" that is shared among all participants of a computation, and can be used to \"broadcast\" or \"shuffle\" the data without any significant overhead.","category":"page"},{"location":"tutorial/#Miscellaneous-functions","page":"Tutorial","title":"Miscellaneous functions","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For convenience, DistributedData also contains simple implementations of various common utility operations for processing matrix data. These originated in flow-cytometry use-cases (which is what DistributedData was originally built for), but are applicable in many other areas of data analysis:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"dselect reduces a matrix to several selected columns (in a relatively usual scenario where the rows of the matrix are \"events\" and columns represent \"features\", dselect discards the unwanted features)\ndapply_cols transforms selected columns with a given function\ndapply_rows does the same with rows\ndstat quickly computes the mean and standard deviation in selected columns (as demonstrated above)\ndstat_buckets does the same for multiple data \"groups\" present in the same matrix, the data groups are specified by a distributed integer vector (This is useful e.g. for computing per-cluster statistics, in which case the integer vector should assign individual data entries to clusters.)\ndcount counts the numbers of occurrences of items in an integer vector, similar to e.g. R function tabulate\ndcount_buckets does the same per groups\ndscale scales the selected columns to mean 0 and standard deviation 1\ndmedian computes a median of the selected columns of the dataset (The computation is done using an approximate iterative algorithm in time O(n*iters), which scales even to really large datasets. The precision of the result increases by roughly 1 bit per iteration, the default is 20 iterations.)\ndmedian_buckets uses the above method to compute the medians for multiple data groups","category":"page"}]
}
