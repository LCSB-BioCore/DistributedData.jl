<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · DistributedData.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://lcsb-biocore.github.io/DistributedData.jl/stable/tutorial/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/logo.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="DistributedData.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">DistributedData.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Moving-the-data-around"><span>Moving the data around</span></a></li><li><a class="tocitem" href="#Transformations-and-reductions"><span>Transformations and reductions</span></a></li><li><a class="tocitem" href="#Persisting-the-data"><span>Persisting the data</span></a></li><li><a class="tocitem" href="#Miscellaneous-functions"><span>Miscellaneous functions</span></a></li></ul></li><li><a class="tocitem" href="../functions/">Function reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/LCSB-BioCore/DistributedData.jl/blob/master/docs/src/tutorial.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="DistributedData-tutorial"><a class="docs-heading-anchor" href="#DistributedData-tutorial">DistributedData tutorial</a><a id="DistributedData-tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#DistributedData-tutorial" title="Permalink"></a></h1><p>The primary purpose of this tutorial is to get a basic grasp of the main <code>DistributedData</code> functions and methodology.</p><p>For starting up, let&#39;s create a few distributed workers and import the package:</p><pre><code class="language-julia hljs">julia&gt; using Distributed, DistributedData

julia&gt; addprocs(3)
2-element Array{Int64,1}:
 2
 3
 4

julia&gt; @everywhere using DistributedData</code></pre><h2 id="Moving-the-data-around"><a class="docs-heading-anchor" href="#Moving-the-data-around">Moving the data around</a><a id="Moving-the-data-around-1"></a><a class="docs-heading-anchor-permalink" href="#Moving-the-data-around" title="Permalink"></a></h2><p>In <code>DistributedData</code>, the storage of distributed data is done in the &quot;native&quot; Julia way – the data is stored in normal named variables. Each node holds its own data in an arbitrary set of variables as &quot;plain data&quot;; content of these variables is completely independent among nodes.</p><p>There are two basic data-moving functions:</p><ul><li><a href="../functions/#DistributedData.save_at-Tuple{Any,Symbol,Any}"><code>save_at</code></a>, which evaluates a given expression on the remote worker, and stores it in a variable. In particular, <code>save_at(3, :x, 123)</code> is roughly the same as if you would manually connect to the Julia session on the worker <code>3</code> and type <code>x = 123</code>.</li><li><a href="../functions/#DistributedData.get_from-Tuple{Any,Any}"><code>get_from</code></a>, which evaluates a given object on the remote worker and returns a <code>Future</code> that holds the evaluated result. To get the value of <code>x</code> from worker <code>3</code>, you may call <code>fetch(get_from(3, :x))</code> to fetch the &quot;contents&quot; of that future. (Additionally, there is <a href="../functions/#DistributedData.get_val_from-Tuple{Any,Any}"><code>get_val_from</code></a>, which calls the <code>fetch</code> for you.)</li></ul><p>The use of these functions is quite straightforward:</p><pre><code class="language-julia hljs">julia&gt; save_at(3,:x,123)
Future(3, 1, 11, nothing)

julia&gt; get_val_from(3, :x)
123

julia&gt; get_val_from(4, :x)
ERROR: On worker 4:
UndefValError: x not defined
…</code></pre><p><code>DistributedData</code> uses <em>quoting</em> to allow you to precisely specify the parts of the code that should be evaluated on the &quot;main&quot; Julia process (the one you interact with), and the code that should be evaluated on the remote workers.  Basically, all quoted code is going to get to the workers without any evaluation; all other code is evaluated on the main node.</p><p>For example, this picks up the contents of variable <code>x</code> from the remote worker, despite that actual symbol is named as <code>y</code> in the main process:</p><pre><code class="language-julia hljs">julia&gt; y=:x
:x

julia&gt; get_val_from(3, y)
123</code></pre><p>This system is used to easily specify that some particular operations (e.g., heavy computations) are going to be executed on the remotes.</p><p>To illustrate the difference between quoted and non-quoted code, the following code generates a huge random matrix locally and sends it to the worker, which may not be desired (the data transfer takes a lot of precious time):</p><pre><code class="language-julia hljs">julia&gt; save_at(2, :mtx, randn(1000, 1000))</code></pre><p>On the remote worker <code>2</code>, this will be executed as something like <code>mtx = [0.384478, 0.763806, -0.885208, …]</code> .</p><p>If you quote the parameter, it is not going to be evaluated on the main worker, but rather goes unevaluated and &quot;packed&quot; as an expression to the remote, which unpacks and evaluates it by itself:</p><pre><code class="language-julia hljs">julia&gt; save_at(2, :mtx, :(randn(1000,1000)))</code></pre><p>The data transfer is minimized to a few-byte expression <code>randn(1000,1000)</code>. On the remote, this is executed properly as <code>mtx = randn(1000, 1000)</code>.</p><p>This is useful for handling large data – you can easily load giant datasets to the workers without hauling all the data through your computer; very likely also decreasing the risk of out-of-memory problems.</p><p>The same principle applies for receiving the data – you can let some of the workers compute a very hard function and download it as follows:</p><pre><code class="language-julia hljs">julia&gt; get_val_from(2, :( computeAnswerToMeaningOfLife() ))
42</code></pre><p>If the expression in the previous case was not quoted, it would actually cause the main worker to compute the answer, send it to worker <code>2</code>, and receive it back unchanged, which is likely not what we wanted.</p><p>Finally, this way it is very easy to work with multiple variables saved at a single worker – you just reference them in the expression:</p><pre><code class="language-julia hljs">julia&gt; save_at(2,:x,123)
julia&gt; save_at(2,:y,321)
julia&gt; get_val_from(2, :(2*x+y))
567</code></pre><h3 id="Parallelization-and-synchronization"><a class="docs-heading-anchor" href="#Parallelization-and-synchronization">Parallelization and synchronization</a><a id="Parallelization-and-synchronization-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelization-and-synchronization" title="Permalink"></a></h3><p>Operations executed by <code>save_at</code> and <code>get_from</code> are <em>asynchronous</em> by default, which may be both good and bad, depending on the situation. For example, when using <code>save_at</code>, the results of hard-to-compute functions may not yet be saved at the time you need them. Let&#39;s demonstrate that on a &quot;simulated&quot; slow function:</p><pre><code class="language-julia hljs">julia&gt; save_at(2, :delayed, :(begin sleep(30); 42; end))
Future(2, 1, 18, nothing)

julia&gt; get_val_from(2, :delayed)      # the computation is not finished yet, thus the variable is not assigned
ERROR: On worker 2:
UndefVarError: delayed not defined

# …wait 30 seconds…

julia&gt; get_val_from(2, :delayed)
42</code></pre><p>The simplest way to prevent such data races is to <code>fetch</code> the future returned from <code>save_at</code>, which correctly waits until the result is properly available on the target worker.</p><p>This <em>synchronization is not performed by default</em>, because the non-synchronized behavior allows you to very easily implement parallelism. In particular, you may start multiple asynchronous computations at once, and then wait for all of them to complete to make sure all results are available. Because the operations run asynchronously, they are processed concurrently, thus faster.</p><p>To illustrate the difference, the following code distributes some random data and then synchronizes correctly, but is essentially serial:</p><pre><code class="language-julia hljs">julia&gt; @time for w in workers()
         fetch(save_at(w, :x, :(randn(10000,10000))))
       end
  1.073267 seconds (346 allocations: 12.391 KiB)</code></pre><p>By spawning the operations first and waiting for all of them later, you can make the code parallel, and usually a few times faster (depending on the number of workers):</p><pre><code class="language-julia hljs">julia&gt; @time map(fetch, [save_at(w, :x, :(randn(10000,10000)))
                         for w in workers()])
  0.403235 seconds (44.50 k allocations: 2.277 MiB)
3-element Array{Nothing,1}:
nothing
…</code></pre><p>The same is applicable for retrieving the sub-results in parallel. This example demonstrates that multiple workers can do some work at the same time:</p><pre><code class="language-julia hljs">julia&gt; @time map(fetch, [get_from(i, :(begin sleep(1); myid(); end))
                         for i in workers()])
  1.027651 seconds (42.26 k allocations: 2.160 MiB)
3-element Array{Int64,1}:
 2
 3
 4</code></pre><p>Notably, you can even send individual <code>Future</code>s to other workers, allowing the workers to synchronize and transfer the data among each other. This is beneficial for implementing advanced parallel algorithms.</p><h3 id="Dinfo-handles"><a class="docs-heading-anchor" href="#Dinfo-handles"><code>Dinfo</code> handles</a><a id="Dinfo-handles-1"></a><a class="docs-heading-anchor-permalink" href="#Dinfo-handles" title="Permalink"></a></h3><p>Remembering and managing the remote variable names and worker numbers is extremely impractical, especially if you need to maintain multiple variables on various subsets of all available workers at once. <code>DistributedData</code> defines a small <a href="../functions/#DistributedData.Dinfo"><code>Dinfo</code></a> data structure that keeps that information for you. Many other functions are able to work with <code>Dinfo</code> transparently, instead of the &quot;raw&quot; symbols and worker lists.</p><p>For example, you can use <a href="../functions/#DistributedData.scatter_array-Tuple{Symbol,Array,Any}"><code>scatter_array</code></a> to automatically separate the array-like dataset to roughly-same pieces scattered across multiple workers, and obtain the <code>Dinfo</code> object:</p><pre><code class="language-julia hljs">julia&gt; dataset = scatter_array(:myData, randn(1000,3), workers())
Dinfo(:myData, [2, 3, 4])</code></pre><p><code>Dinfo</code> contains the necessary information about the &quot;contents&quot; of the distributed dataset: The name of variable used to save it on workers, and IDs of individual workers. The storage of the variables is otherwise same as with the basic data-moving function – you can e.g. manually check the size of the resulting slices on each worker using <code>get_from</code>:</p><pre><code class="language-julia hljs">julia&gt; map(fetch, [get_from(w, :(size($(dataset.val))))
                   for w in dataset.workers])
3-element Array{Tuple{Int64,Int64},1}:
 (333, 3)
 (333, 3)
 (334, 3)</code></pre><p>(Note the <code>$(...)</code> syntax for <em>un-quoting</em>, i.e., inserting evaluated data into quoted expressions.)</p><p>The <code>Dinfo</code> object is used e.g. by the statistical functions, such as <a href="../functions/#DistributedData.dstat-Tuple{Dinfo,Array{Int64,1}}"><code>dstat</code></a> (see below for more examples). <code>dstat</code> just computes means and standard deviations in selected columns of the data:</p><pre><code class="language-julia hljs">julia&gt; dstat(dataset, [1,2])
([-0.029108965193981328, 0.019687519297162222],     # means
 [0.9923669075507301, 0.9768313338000191])          # sdevs</code></pre><p>There are three functions for straightforward data management using the <code>Dinfo</code>:</p><ul><li><a href="../functions/#DistributedData.dcopy-Tuple{Dinfo,Symbol}"><code>dcopy</code></a> for duplicating the data objects on all related workers</li><li><a href="../functions/#DistributedData.unscatter-Tuple{Dinfo}"><code>unscatter</code></a> for removing the data from workers (and freeing the memory)</li><li><a href="../functions/#DistributedData.gather_array"><code>gather_array</code></a> for collecting the array pieces from individual workers and pasting them together (an opposite of <code>scatter_array</code>)</li></ul><p>Continuing the previous example, we can copy the data, remove the originals, and gather the copies:</p><pre><code class="language-julia hljs">julia&gt; dataset2 = dcopy(dataset, :backup)
Dinfo(:backup, [2, 3, 4])

julia&gt; unscatter(dataset)

julia&gt; get_val_from(2, :myData)
 # nothing

julia&gt; gather_array(dataset2)
1000×3 Array{Float64,2}:
  0.241102   0.62638     0.759203
  0.981085  -1.01467    -0.495331
 -0.439979  -0.884943   -1.62218
  ⋮</code></pre><h2 id="Transformations-and-reductions"><a class="docs-heading-anchor" href="#Transformations-and-reductions">Transformations and reductions</a><a id="Transformations-and-reductions-1"></a><a class="docs-heading-anchor-permalink" href="#Transformations-and-reductions" title="Permalink"></a></h2><p>There are several simplified functions to run parallel computation on the distributed data:</p><ul><li><a href="../functions/#DistributedData.dtransform"><code>dtransform</code></a> processes all worker&#39;s parts of the data using a given function and stores the result</li><li><a href="../functions/#DistributedData.dmapreduce-NTuple{4,Any}"><code>dmapreduce</code></a> applies (&quot;maps&quot;) a function to all data parts and reduces (&quot;folds&quot;) the intermediate results to a single result using another function</li><li><a href="../functions/#DistributedData.dexec-Tuple{Any,Any,Any}"><code>dexec</code></a> is similar to <code>dtransform</code>, but expects a function that modifies the data in-place (using &quot;side-effects&quot;), for increased efficiency in cases such as very small array modifications</li><li><a href="../functions/#DistributedData.dmap-Tuple{Array{T,1} where T,Any,Any}"><code>dmap</code></a> executes a function over the workers, also distributing a vector of values as parameters for that function.</li></ul><p>For example, <code>dtransform</code> can be used to exponentiate the whole dataset:</p><pre><code class="language-julia hljs">julia&gt; dataset = scatter_array(:myData, randn(1000,3), workers())
julia&gt; get_val_from(dataset.workers[1], :(myData[1,:]))
3-element Array{Float64,1}:
 -1.0788051465727018
 -0.29710863020942757
 -2.5613834309426546

julia&gt; dtransform(dataset, x -&gt; 2 .^ x)
Dinfo(:myData, [2, 3, 4])

julia&gt; get_val_from(dataset.workers[1], :(myData[1,:]))
3-element Array{Float64,1}:
 0.4734207525033287
 0.8138819001228813
 0.16941300928000705</code></pre><p>You may have noticed that <code>dtransform</code> returns a new <code>Dinfo</code> object, which we safely discard in this case. You can use <code>dtransform</code> to save the result into another distributed variable (by supplying the new name in an extra argument), in which case the returned <code>Dinfo</code> wraps this new distributed variable. That is useful for easily generating new datasets on all workers, as in the following example:</p><pre><code class="language-julia hljs">julia&gt; anotherDataset = dtransform((), _ -&gt;randn(100), workers(), :newData)
Dinfo(:newData, [2, 3, 4])</code></pre><p>(Note that the function body does not need to be quoted.)</p><p>The <code>dexec</code> function is handy if your transformation does not modify the whole array, but leaves most of it untouched and rewriting it would be a waste of resources. This example multiplies the 5th element of each distributed array part by 42:</p><pre><code class="language-julia hljs">julia&gt; dexec(anotherDataset, arr -&gt; arr[5] *= 42)
julia&gt; gather_array(anotherDataset)[1:6]
6-element Array{Float64,1}:
  0.8270400003123709
 -0.10688512653581493
 -1.0462015551052068
 -1.2891453384843214
 16.429315504503112
  0.13958421716454797
  ⋮</code></pre><p>MapReduce is a handy primitive that is suitable for operations that can &quot;compress&quot; the dataset slices into relatively small pieces of data, which can be combined efficiently. For example, this computes the sum of squares of the whole array:</p><pre><code class="language-julia hljs">julia&gt; dmapreduce(anotherDataset, x -&gt; sum(x.^2), +)
8633.94032741762</code></pre><p>Finally, <code>dmap</code> passes each worker a specific value from a given vector, which may be useful in cases when each worker is supposed to do something slightly different with the data (for example, submit them to a different interface or save them as a different file). The results are returned as a vector. This example is rather simplistic:</p><pre><code class="language-julia hljs">julia&gt; dmap(Vector(1:length(workers())),
                   val -&gt; &quot;Worker number $(val) has ID $(myid())&quot;,
		   workers())
3-element Array{String,1}:
 &quot;Worker number 1 has ID 2&quot;
 &quot;Worker number 2 has ID 3&quot;
 &quot;Worker number 3 has ID 4&quot;</code></pre><h2 id="Persisting-the-data"><a class="docs-heading-anchor" href="#Persisting-the-data">Persisting the data</a><a id="Persisting-the-data-1"></a><a class="docs-heading-anchor-permalink" href="#Persisting-the-data" title="Permalink"></a></h2><p><code>DistributedData</code> provides support for storing the loaded dataset in each worker&#39;s local storage. This is quite beneficial for saving sub-results and various artifacts of the computation process for later use, without unnecessarily wasting main memory.</p><p>The available functions are as follows:</p><ul><li><a href="../functions/#DistributedData.dstore"><code>dstore</code></a> saves the dataset to a disk, such as in <code>dstore(anotherDataset)</code>, which, in this case, creates files <code>newData-1.slice</code> to <code>newData-3.slice</code> that contain the respective parts of the dataset. The precise naming scheme can be specified using the <code>files</code> parameter.</li><li><a href="../functions/#DistributedData.dload"><code>dload</code></a> loads the data back into memory (again using a <code>Dinfo</code> parameter with dataset description to get the dataset name and the list of relevant workers)</li><li><a href="../functions/#DistributedData.dunlink"><code>dunlink</code></a> removes the corresponding files from the storage</li></ul><p>Apart from saving the data for later use, this provides a relatively easy way to exchange the data among nodes in a HPC environment. There, the disk storage is usually a very fast &quot;scratch space&quot; that is shared among all participants of a computation, and can be used to &quot;broadcast&quot; or &quot;shuffle&quot; the data without any significant overhead.</p><h2 id="Miscellaneous-functions"><a class="docs-heading-anchor" href="#Miscellaneous-functions">Miscellaneous functions</a><a id="Miscellaneous-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Miscellaneous-functions" title="Permalink"></a></h2><p>For convenience, <code>DistributedData</code> also contains simple implementations of various common utility operations for processing matrix data. These originated in flow-cytometry use-cases (which is what <code>DistributedData</code> was originally built for), but are applicable in many other areas of data analysis:</p><ul><li><a href="../functions/#DistributedData.dselect"><code>dselect</code></a> reduces a matrix to several selected columns (in a relatively usual scenario where the rows of the matrix are &quot;events&quot; and columns represent &quot;features&quot;, <code>dselect</code> discards the unwanted features)</li><li><a href="../functions/#DistributedData.dapply_cols-Tuple{Dinfo,Any,Array{Int64,1}}"><code>dapply_cols</code></a> transforms selected columns with a given function</li><li><a href="../functions/#DistributedData.dapply_rows-Tuple{Dinfo,Any}"><code>dapply_rows</code></a> does the same with rows</li><li><a href="../functions/#DistributedData.dstat-Tuple{Dinfo,Array{Int64,1}}"><code>dstat</code></a> quickly computes the mean and standard deviation in selected columns (as demonstrated above)</li><li><a href="../functions/#DistributedData.dstat_buckets-Tuple{Dinfo,Int64,Dinfo,Array{Int64,1}}"><code>dstat_buckets</code></a> does the same for multiple data &quot;groups&quot; present in the same matrix, the data groups are specified by a distributed integer vector (This is useful e.g. for computing per-cluster statistics, in which case the integer vector should assign individual data entries to clusters.)</li><li><a href="../functions/#DistributedData.dcount-Tuple{Int64,Dinfo}"><code>dcount</code></a> counts the numbers of occurrences of items in an integer vector, similar to e.g. R function <code>tabulate</code></li><li><a href="../functions/#DistributedData.dcount_buckets-Tuple{Int64,Dinfo,Int64,Dinfo}"><code>dcount_buckets</code></a> does the same per groups</li><li><a href="../functions/#DistributedData.dscale-Tuple{Dinfo,Array{Int64,1}}"><code>dscale</code></a> scales the selected columns to mean 0 and standard deviation 1</li><li><a href="../functions/#DistributedData.dmedian-Tuple{Dinfo,Array{Int64,1}}"><code>dmedian</code></a> computes a median of the selected columns of the dataset (The computation is done using an approximate iterative algorithm in time <code>O(n*iters)</code>, which scales even to really large datasets. The precision of the result increases by roughly 1 bit per iteration, the default is 20 iterations.)</li><li><a href="../functions/#DistributedData.dmedian_buckets-Tuple{Dinfo,Int64,Dinfo,Array{Int64,1}}"><code>dmedian_buckets</code></a> uses the above method to compute the medians for multiple data groups</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Documentation</a><a class="docs-footer-nextpage" href="../functions/">Function reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.15 on <span class="colophon-date" title="Tuesday 22 March 2022 13:58">Tuesday 22 March 2022</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
