<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>DistributedData.jl on Slurm clusters · DistributedData.jl</title><link rel="canonical" href="https://lcsb-biocore.github.io/DistributedData.jl/stable/slurm/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/logo.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="DistributedData.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DistributedData.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../functions/">Function reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>DistributedData.jl on Slurm clusters</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>DistributedData.jl on Slurm clusters</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/LCSB-BioCore/DistributedData.jl/blob/master/docs/src/slurm.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="DistributedData.jl-on-Slurm-clusters"><a class="docs-heading-anchor" href="#DistributedData.jl-on-Slurm-clusters">DistributedData.jl on Slurm clusters</a><a id="DistributedData.jl-on-Slurm-clusters-1"></a><a class="docs-heading-anchor-permalink" href="#DistributedData.jl-on-Slurm-clusters" title="Permalink"></a></h1><p>DistributedData scales well to moderately large computer clusters. As a practical example, you can see the script that was used to process relatively large datasets for GigaSOM, documented in the Supplementary file of the <a href="https://academic.oup.com/gigascience/article/9/11/giaa127/5987271">GigaSOM article</a> (click the <code>giaa127_Supplemental_File</code> link below on the page, and find <em>Listing S1</em> at the end of the PDF).</p><p>Use of DistributedData with <a href="https://slurm.schedmd.com/overview.html">Slurm</a> is similar as with many other distributed computing systems:</p><ol><li>You install Julia and the required packages.</li><li>You submit a batch (or interactive) task, which runs your Julia script on a node and gives it some information about where to find other worker nodes.</li><li>In the Julia script, you use <a href="https://github.com/JuliaParallel/ClusterManagers.jl"><code>ClusterManagers</code></a> function <code>addprocs_slurm</code> to add the processes, just as with normal <code>addprocs</code>. Similar functions exist for many other task schedulers, including the popular PBS and LSF.</li><li>The rest of the workflow is unchanged; all functions from <code>DistributedData</code> such as <code>save_at</code> and <code>dmapreduce</code> will work in the cluster just as they worked locally. Performance will vary though – you may want to optimize your algorithm to use as much parallelism as possible (to get lots of performance), load more data in the memory (usually, much more total memory is available in the clusters than on a single computer), but keep an eye on the communication overhead, transferring only the minimal required amount of data as seldom as possible.</li></ol><h3 id="Preparing-the-packages"><a class="docs-heading-anchor" href="#Preparing-the-packages">Preparing the packages</a><a id="Preparing-the-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Preparing-the-packages" title="Permalink"></a></h3><p>The easiest way to install the packages is using a single-machine interactive job. On the access node of your HPC, run this command to give you a 60-minute interactive session:</p><pre><code class="language-sh">srun --pty -t60 /bin/bash -</code></pre><p>(Depending on your cluster setup, it may be benerifical to also specify a partition to which the job should belong to; many clusters provide an interactive partition where the interactive jobs get scheduled faster. To do that, add option <code>-p interactive</code> into the parameters of <code>srun</code>.)</p><p>When the shell opens (the prompt should change), you can load the Julia module, usually with a command such as this:</p><pre><code class="language-sh">module load lang/Julia/1.3.0</code></pre><p>(You may consult <code>module spider julia</code> for other possible Julia versions.)</p><p>After that, start <code>julia</code> and add press <code>]</code> to open the packaging prompt (you should see <code>(v1.3) pkg&gt;</code> instead of <code>julia&gt;</code>). There you can download and install the required packages:</p><pre><code class="language-none">add DistributedData, ClusterManagers</code></pre><p>You may also want to load the packages to precompile them, which saves precious time later in the workflows. Press backspace to return to the &quot;normal&quot; Julia shell, and type:</p><pre><code class="language-julia">using DistributedData, ClusterManagers</code></pre><p>Depending on the package, this may take a while, but should be done in under a minute for most existing packages. Finally, press <code>Ctrl+D</code> twice to exit both Julia and the interactive Slurm job shell.</p><h3 id="Slurm-batch-script"><a class="docs-heading-anchor" href="#Slurm-batch-script">Slurm batch script</a><a id="Slurm-batch-script-1"></a><a class="docs-heading-anchor-permalink" href="#Slurm-batch-script" title="Permalink"></a></h3><p>An example Slurm batch script (<a href="https://github.com/LCSB-BioCore/DistributedData.jl/blob/master/docs/slurm-example/run-analysis.batch">download</a>) is listed below – save it as <code>run-analysis.batch</code> to your Slurm access node, in a directory that is shared with the workers (usually a &quot;scratch&quot; directory; try <code>cd $SCRATCH</code>).</p><pre><code class="language-sh">#!/bin/bash -l
#SBATCH -n 128
#SBATCH -c 1
#SBATCH -t 10
#SBATCH --mem-per-cpu 4G
#SBATCH -J MyDistributedJob

module load lang/Julia/1.3.0

julia run-analysis.jl</code></pre><p>The parameters in the script have this meaning, in order:</p><ul><li>the batch spawns 128 &quot;tasks&quot; (ie. spawning 128 separate processes)</li><li>each task uses 1 CPU (you may want more CPUs if you work with actual threads and shared memory)</li><li>the whole batch takes maximum 10 minutes</li><li>each CPU (in our case each process) will be allocated 4 gigabytes of RAM</li><li>the job will be visible in the queue as <code>MyDistributedJob</code></li><li>it will load Julia 1.3.0 module on the workers, so that <code>julia</code> executable is available (you may want to consult the versions availability with your HPC administrators)</li><li>finally, it will run the Julia script <code>run-analysis.jl</code></li></ul><h3 id="Julia-script"><a class="docs-heading-anchor" href="#Julia-script">Julia script</a><a id="Julia-script-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-script" title="Permalink"></a></h3><p>The <code>run-analysis.jl</code> (<a href="https://github.com/LCSB-BioCore/DistributedData.jl/blob/master/docs/slurm-example/run-analysis.jl">download</a>) may look as follows:</p><pre><code class="language-julia">using  Distributed, ClusterManagers, DistributedData

# read the number of available workers from  environment and start the worker processes
n_workers = parse(Int , ENV[&quot;SLURM_NTASKS&quot;])
addprocs_slurm(n_workers , topology =:master_worker)

# load the required packages on all workers
@everywhere using DistributedData

# generate a random dataset on all workers
dataset = dtransform((), _ -&gt; randn(10000,10000), workers(), :myData)

# for demonstration, sum the whole dataset
totalResult = dmapreduce(dataset, sum, +)

# do not forget to save the results!
f = open(&quot;result.txt&quot;, &quot;w&quot;)
println(f, totalResult)
close(f)</code></pre><p>Finally, you can start the whole thing with <code>sbatch</code> command executed on the access node:</p><pre><code class="language-sh">sbatch run-analysis.batch</code></pre><h3 id="Collecting-the-results"><a class="docs-heading-anchor" href="#Collecting-the-results">Collecting the results</a><a id="Collecting-the-results-1"></a><a class="docs-heading-anchor-permalink" href="#Collecting-the-results" title="Permalink"></a></h3><p>After your tasks gets queued, executed and finished successfully, you may see the result in <code>result.txt</code>. In the meantime, you can entertain yourself by watching <code>squeue</code>, to see e.g. the expected execution time of your batch.</p><p>Note that you may want to run the analysis in a separate directory, because the logs from all workers are collected in the current path by default. The resulting files may look like this:</p><pre><code class="language-none">0 [user@access1 test]$ ls
job0000.out  job0019.out  job0038.out  job0057.out  job0076.out  job0095.out  job0114.out
job0001.out  job0020.out  job0039.out  job0058.out  job0077.out  job0096.out  job0115.out
job0002.out  job0021.out  job0040.out  job0059.out  job0078.out  job0097.out  job0116.out
job0003.out  job0022.out  job0041.out  job0060.out  job0079.out  job0098.out  job0117.out
job0004.out  job0023.out  job0042.out  job0061.out  job0080.out  job0099.out  job0118.out
job0005.out  job0024.out  job0043.out  job0062.out  job0081.out  job0100.out  job0119.out
job0006.out  job0025.out  job0044.out  job0063.out  job0082.out  job0101.out  job0120.out
job0007.out  job0026.out  job0045.out  job0064.out  job0083.out  job0102.out  job0121.out
job0008.out  job0027.out  job0046.out  job0065.out  job0084.out  job0103.out  job0122.out
job0009.out  job0028.out  job0047.out  job0066.out  job0085.out  job0104.out  job0123.out
job0010.out  job0029.out  job0048.out  job0067.out  job0086.out  job0105.out  job0124.out
job0011.out  job0030.out  job0049.out  job0068.out  job0087.out  job0106.out  job0125.out
job0012.out  job0031.out  job0050.out  job0069.out  job0088.out  job0107.out  job0126.out
job0013.out  job0032.out  job0051.out  job0070.out  job0089.out  job0108.out  job0127.out
job0014.out  job0033.out  job0052.out  job0071.out  job0090.out  job0109.out  result.txt        &lt;-- here is the result!
job0015.out  job0034.out  job0053.out  job0072.out  job0091.out  job0110.out  run-analysis.jl
job0016.out  job0035.out  job0054.out  job0073.out  job0092.out  job0111.out  run-analysis.sbatch
job0017.out  job0036.out  job0055.out  job0074.out  job0093.out  job0112.out  slurm-2237171.out
job0018.out  job0037.out  job0056.out  job0075.out  job0094.out  job0113.out</code></pre><p>The files <code>job*.out</code> contain the information collected from individual workers&#39; standard outputs, such as the output of <code>println</code> or <code>@info</code>. For complicated programs, this is the easiest way to get out the debugging information, and a simple but often sufficient way to collect benchmarking output (using e.g. <code>@time</code>).</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 24 February 2021 09:39">Wednesday 24 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
